import yfinance as yf
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import accuracy_score, classification_report
from scipy.stats import zscore
import joblib
import warnings
warnings.filterwarnings("ignore")

# ğŸŸ¢ 1. ë°ì´í„° ë‹¤ìš´ë¡œë“œ
spy = yf.download("SPY", start="2018-01-01", end="2025-04-30", auto_adjust=False)
qqq = yf.download("QQQ", start="2018-01-01", end="2025-04-30", auto_adjust=False)

if isinstance(spy.columns, pd.MultiIndex):
    spy.columns = spy.columns.droplevel(1)
if isinstance(qqq.columns, pd.MultiIndex):
    qqq.columns = qqq.columns.droplevel(1)

# ğŸŸ¢ 2. ì—´ í•„í„°ë§ ë° ì ‘ë‘ì‚¬ ì¶”ê°€
def prepare(df, name):
    df = df[['Open', 'High', 'Low', 'Close', 'Volume']]
    df.columns = [f"{name}_{col}" for col in df.columns]
    return df

spy = prepare(spy, "SPY")
qqq = prepare(qqq, "QQQ")
df = pd.concat([spy, qqq], axis=1)

# âœ… 3. ê²°ì¸¡ì¹˜ í™•ì¸ ë° ì œê±°
print("\n[ê²°ì¸¡ì¹˜ ê°œìˆ˜]")
print(df.isnull().sum())
df.dropna(inplace=True)

# âœ… 4. ìˆ˜ìµë¥  ë° íƒ€ê²Ÿ
df['SPY_return'] = df['SPY_Close'].pct_change()
df['QQQ_return'] = df['QQQ_Close'].pct_change()
df['Target'] = (df['SPY_return'].shift(-1) > 0).astype(int)
df.dropna(inplace=True)

# âœ… 5. ì´ìƒì¹˜ ì œê±° (ê±°ë˜ëŸ‰ ê¸°ì¤€ z-score)
for col in ['SPY_Volume', 'QQQ_Volume']:
    df[f'{col}_z'] = zscore(df[col])
df = df[(df['SPY_Volume_z'].abs() < 3) & (df['QQQ_Volume_z'].abs() < 3)]
df.drop(columns=['SPY_Volume_z', 'QQQ_Volume_z'], inplace=True)

# âœ… 6. ìƒê´€ê´€ê³„ ì‹œê°í™”
cor = df[['SPY_return', 'QQQ_return', 'SPY_Close', 'QQQ_Close']].corr()
sns.heatmap(cor, annot=True, cmap='coolwarm')
plt.title("Correlation Heatmap")
plt.savefig("eda_corr_heatmap.png")
plt.close()

# âœ… 7. íŠ¹ì„± ì„ íƒ
X = df[['SPY_Close', 'SPY_Volume', 'QQQ_Close', 'SPY_return', 'QQQ_return']]
y = df["Target"]
X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=False, test_size=0.2)

# âœ… 8. ëª¨ë¸ë³„ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ë° í‰ê°€
results = {}

# Logistic Regression
logit = LogisticRegression(max_iter=500)
logit_params = {'C': [0.01, 0.1, 1, 10]}
logit_grid = GridSearchCV(logit, logit_params, cv=3)
logit_grid.fit(X_train, y_train)
logit_best = logit_grid.best_estimator_
logit_acc = accuracy_score(y_test, logit_best.predict(X_test))
results['Logistic Regression'] = (logit_best, logit_acc)
print("\nğŸ“Œ Logistic Regression ê²°ê³¼:")
print(classification_report(y_test, logit_best.predict(X_test)))

# Random Forest
rf = RandomForestClassifier()
rf_params = {'n_estimators': [100, 150], 'max_depth': [5, 10, 15]}
rf_grid = GridSearchCV(rf, rf_params, cv=3)
rf_grid.fit(X_train, y_train)
rf_best = rf_grid.best_estimator_
rf_acc = accuracy_score(y_test, rf_best.predict(X_test))
results['Random Forest'] = (rf_best, rf_acc)
print("\nğŸ“Œ Random Forest ê²°ê³¼:")
print(classification_report(y_test, rf_best.predict(X_test)))

# Gradient Boosting
gb = GradientBoostingClassifier()
gb_params = {'n_estimators': [100, 150], 'learning_rate': [0.01, 0.1]}
gb_grid = GridSearchCV(gb, gb_params, cv=3)
gb_grid.fit(X_train, y_train)
gb_best = gb_grid.best_estimator_
gb_acc = accuracy_score(y_test, gb_best.predict(X_test))
results['Gradient Boosting'] = (gb_best, gb_acc)
print("\nğŸ“Œ Gradient Boosting ê²°ê³¼:")
print(classification_report(y_test, gb_best.predict(X_test)))

# âœ… 9. ìµœì  ëª¨ë¸ ì„ íƒ ë° ì €ì¥
best_model_name, (best_model, best_acc) = max(results.items(), key=lambda x: x[1][1])
joblib.dump(best_model, "etf_rf_model.pkl")
print(f"\nâœ… ìµœì¢… ì„ íƒ ëª¨ë¸: {best_model_name} (ì •í™•ë„: {best_acc:.4f}) ì €ì¥ ì™„ë£Œ")
